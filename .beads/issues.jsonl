{"id":"TO-017","title":"Install project packages (editable)","description":"pip install -e tau2-bench and training/rollout","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:42.947832361-06:00","created_by":"kp","updated_at":"2026-01-17T02:38:32.772038106-06:00","closed_at":"2026-01-17T02:38:32.772038106-06:00","close_reason":"Project packages installed: tau2, verl modules import successfully. CLI requires REPO_PATH env var which is configured in bashrc.","dependencies":[{"issue_id":"TO-017","depends_on_id":"TO-jeu","type":"blocks","created_at":"2026-01-15T14:44:42.948977045-06:00","created_by":"kp"}]}
{"id":"TO-11e","title":"Final verification","description":"Comprehensive CUDA 13.0 verification across all 3 environments: toolorchestra (PyTorch, Flash Attention, vLLM, CuPy, tau2, verl), vllm1 (PyTorch, Flash Attention, vLLM), retriever (already working). Tests GPU memory, CUDA kernels, and environment variables.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-15T14:44:43.195379218-06:00","created_by":"kp","updated_at":"2026-01-17T02:42:04.517822826-06:00","closed_at":"2026-01-17T02:42:04.517822826-06:00","close_reason":"All verification passed: toolorchestra (PyTorch 2.9.1 CUDA 13.0, Flash Attention, vLLM, CuPy, Core ML, tau2/verl), vllm1 (PyTorch CUDA 13.0, Flash Attention, vLLM), retriever (PyTorch CUDA 13.0, FAISS). GPU computation and Flash Attention kernels verified. Env vars in ~/.profile.","dependencies":[{"issue_id":"TO-11e","depends_on_id":"TO-jeu","type":"blocks","created_at":"2026-01-17T02:21:23.235974438-06:00","created_by":"daemon"},{"issue_id":"TO-11e","depends_on_id":"TO-yb6","type":"blocks","created_at":"2026-01-17T02:21:24.683793976-06:00","created_by":"daemon"}]}
{"id":"TO-16s","title":"Install PyTorch CUDA 13.0 in toolorchestra","description":"Install PyTorch 2.9.1, torchvision 0.24.1, torchaudio 2.9.1 from Jetson AI Lab mirror (https://pypi.jetson-ai-lab.io/sbsa/cu130/) in toolorchestra environment. This replaces the CPU-only PyTorch currently installed.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-17T02:20:40.802019796-06:00","updated_at":"2026-01-17T02:23:30.247537358-06:00","closed_at":"2026-01-17T02:23:30.247537358-06:00","close_reason":"PyTorch 2.9.1 with CUDA 13.0 installed and verified. Device: NVIDIA Thor, Compute Capability: (11, 0)"}
{"id":"TO-1yz","title":"Add detect_platform() to ralph-init","description":"Detect CPU architecture (uname -m), JetPack version (/etc/nv_tegra_release), CUDA version, path, cuDNN, GPU name/memory/compute capability, driver version, OS distribution and version.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T15:12:45.685219453-06:00","updated_at":"2026-01-17T15:15:39.251002187-06:00","closed_at":"2026-01-17T15:15:39.251002187-06:00","close_reason":"Implemented detect_platform() with CPU arch, JetPack, CUDA, GPU, cuDNN detection"}
{"id":"TO-2lp","title":"Build vLLM from source","description":"Clone and build vLLM v0.8.5 for ARM64","acceptance_criteria":"from vllm import LLM works","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:44:28.585293358-06:00","created_by":"kp","updated_at":"2026-01-15T19:56:35.631578344-06:00","closed_at":"2026-01-15T19:56:35.631578344-06:00","close_reason":"vLLM v0.14.0rc2 built from main branch for CUDA 13.0 / PyTorch 2.9.1"}
{"id":"TO-5w4","title":"Create retriever conda environment","description":"Create env with Python 3.12, PyTorch, FAISS, and retrieval packages","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:43.012680912-06:00","created_by":"kp","updated_at":"2026-01-15T20:57:52.343726929-06:00","closed_at":"2026-01-15T20:57:52.343726929-06:00","close_reason":"Retriever environment created with faiss-cpu, pyserini, tavily, torch 2.9.1, transformers 4.57.5"}
{"id":"TO-7r2","title":"Add detect_requirements() to ralph-init","description":"Check for requirements.md in project root, specs/requirements.md, docs/requirements.md, PRD.md. Parse ## sections for objectives, tech stack, dependencies.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T15:12:47.334631733-06:00","updated_at":"2026-01-17T15:15:40.770917062-06:00","closed_at":"2026-01-17T15:15:40.770917062-06:00","close_reason":"Implemented detect_requirements() with requirements.md parsing","dependencies":[{"issue_id":"TO-7r2","depends_on_id":"TO-1yz","type":"blocks","created_at":"2026-01-17T15:13:13.692213337-06:00","created_by":"daemon"}]}
{"id":"TO-7wd","title":"Generate platform-aware prd.json","description":"Combine platform detection results, include parsed requirements, select appropriate package_index based on platform (Jetson wheels, PyTorch CUDA wheels, or PyPI). Generate comprehensive JSON with jq.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T15:12:50.176701163-06:00","updated_at":"2026-01-17T15:15:43.779865682-06:00","closed_at":"2026-01-17T15:15:43.779865682-06:00","close_reason":"Updated prd.json with comprehensive platform and requirements data","dependencies":[{"issue_id":"TO-7wd","depends_on_id":"TO-1yz","type":"blocks","created_at":"2026-01-17T15:13:15.126652907-06:00","created_by":"daemon"},{"issue_id":"TO-7wd","depends_on_id":"TO-7r2","type":"blocks","created_at":"2026-01-17T15:13:16.558438167-06:00","created_by":"daemon"}]}
{"id":"TO-844","title":"Integrate code2prompt for prompt.md generation","description":"Use code2prompt to generate intelligent prompt.md describing the codebase structure, entry points, and imports. Check if code2prompt is available, run on project directory, generate .ralph/prompt.md.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-17T15:12:48.776106478-06:00","updated_at":"2026-01-17T15:15:42.27849923-06:00","closed_at":"2026-01-17T15:15:42.27849923-06:00","close_reason":"Integrated code2prompt for prompt.md generation with fallback"}
{"id":"TO-92i","title":"Fix conda pip path issue and install PyTorch","description":"Install PyTorch with CUDA support for Jetson Thor. Use explicit conda pip path to avoid system pip.","acceptance_criteria":"torch.cuda.is_available() returns True","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-15T14:44:28.461863514-06:00","created_by":"kp","updated_at":"2026-01-15T14:58:56.807261291-06:00","closed_at":"2026-01-15T14:58:56.807261291-06:00","close_reason":"PyTorch 2.9.1+cu130 installed successfully. CUDA available: True. GPU: NVIDIA Thor"}
{"id":"TO-cei","title":"Update ORCHESTRATION_ESSENTIALS.md with new capabilities","description":"Document new platform detection, requirements parsing, and code2prompt integration capabilities.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-17T15:12:53.054807149-06:00","updated_at":"2026-01-17T15:19:59.186643024-06:00","closed_at":"2026-01-17T15:19:59.186643024-06:00","close_reason":"Updated ORCHESTRATION_ESSENTIALS.md with v2.1 documentation"}
{"id":"TO-cet","title":"Fix PyTorch CUDA 13.0 in vllm1 environment","description":"Uninstall CPU PyTorch, install PyTorch 2.9.1, torchvision 0.24.1, torchaudio 2.9.1 with CUDA 13.0 from Jetson AI Lab mirror in vllm1 environment.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-17T02:20:42.266262186-06:00","updated_at":"2026-01-17T02:23:31.721373915-06:00","closed_at":"2026-01-17T02:23:31.721373915-06:00","close_reason":"PyTorch 2.9.1 with CUDA 13.0 installed and verified in vllm1. Device: NVIDIA Thor"}
{"id":"TO-jeu","title":"Install core ML packages in toolorchestra","description":"Install core ML packages with CuPy 13.6.0 from Jetson AI Lab mirror, and filtered requirements.txt (excluding torch, flash-attn, vllm, cupy, langfuse, xformers, nvidia-*). Packages: transformers, accelerate, datasets, ray, wandb, hydra-core.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:28.646265203-06:00","created_by":"kp","updated_at":"2026-01-17T02:35:40.396624886-06:00","closed_at":"2026-01-17T02:35:40.396624886-06:00","close_reason":"Core ML packages verified: transformers 4.57.5, accelerate 1.12.0, datasets 4.5.0, ray 2.52.1, wandb 0.24.0, hydra 1.3.2, cupy-cuda12x 13.6.0","dependencies":[{"issue_id":"TO-jeu","depends_on_id":"TO-mjt","type":"blocks","created_at":"2026-01-17T02:21:21.783445554-06:00","created_by":"daemon"}]}
{"id":"TO-ky4","title":"Build Flash Attention from source","description":"Clone and build flash-attn for ARM64 SM 90","acceptance_criteria":"from flash_attn import flash_attn_func works","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:28.525186603-06:00","created_by":"kp","updated_at":"2026-01-15T20:00:07.186069592-06:00","closed_at":"2026-01-15T20:00:07.186069592-06:00","close_reason":"Flash Attention v2.7.4.post1 built from source for CUDA 13.0"}
{"id":"TO-mjt","title":"Install vLLM 0.13.0+cu130 in toolorchestra and vllm1","description":"Uninstall old vLLM from vllm1 (v0.10.0), install pre-built vllm==0.13.0+cu130 from Jetson AI Lab mirror in both toolorchestra and vllm1 environments.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T02:20:45.178082491-06:00","updated_at":"2026-01-17T02:28:12.659542236-06:00","closed_at":"2026-01-17T02:28:12.659542236-06:00","close_reason":"vLLM 0.13.0+cu130 installed and verified in both toolorchestra and vllm1 with CUDA support","dependencies":[{"issue_id":"TO-mjt","depends_on_id":"TO-rwb","type":"blocks","created_at":"2026-01-17T02:21:20.281431708-06:00","created_by":"daemon"}]}
{"id":"TO-qff","title":"Create vllm1 conda environment","description":"Create inference env with transformers\u003c4.54 for tau2-bench","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:43.073061716-06:00","created_by":"kp","updated_at":"2026-01-15T20:43:24.147256597-06:00","closed_at":"2026-01-15T20:43:24.147256597-06:00","close_reason":"vllm1 environment created with PyTorch 2.7.0, transformers 4.53.3, vLLM 0.10.0"}
{"id":"TO-rwb","title":"Install Flash Attention 2.8.4 in toolorchestra and vllm1","description":"Uninstall existing flash-attn (built against CPU PyTorch), install pre-built flash-attn==2.8.4 from Jetson AI Lab mirror in both toolorchestra and vllm1 environments.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T02:20:43.727364784-06:00","updated_at":"2026-01-17T02:25:15.71735816-06:00","closed_at":"2026-01-17T02:25:15.71735816-06:00","close_reason":"Flash Attention 2.8.4 installed and verified in both toolorchestra and vllm1 with CUDA support","dependencies":[{"issue_id":"TO-rwb","depends_on_id":"TO-16s","type":"blocks","created_at":"2026-01-17T02:21:17.295024881-06:00","created_by":"daemon"},{"issue_id":"TO-rwb","depends_on_id":"TO-cet","type":"blocks","created_at":"2026-01-17T02:21:18.780108118-06:00","created_by":"daemon"}]}
{"id":"TO-vmc","title":"Add adapt_to_platform() to ralph-loop","description":"Read platform from prd.json at startup, set PIP_INSTALL based on is_jetson/architecture, set LOW_MEMORY_MODE based on GPU memory, export platform-specific environment variables.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T15:12:51.612009325-06:00","updated_at":"2026-01-17T15:18:50.744104-06:00","closed_at":"2026-01-17T15:18:50.744104-06:00","close_reason":"Added adapt_to_platform() with PIP_INSTALL, LOW_MEMORY_MODE, and context loading","dependencies":[{"issue_id":"TO-vmc","depends_on_id":"TO-7wd","type":"blocks","created_at":"2026-01-17T15:13:17.973897484-06:00","created_by":"daemon"}]}
{"id":"TO-xey","title":"RALPH-BOOT: Requirements-Driven Project Bootstrap","description":"Enable ralph-init and ralph-loop to create and complete new projects from requirements.md by detecting platform capabilities and parsing project specifications.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-17T15:12:25.537074264-06:00","updated_at":"2026-01-17T15:23:05.335620896-06:00","closed_at":"2026-01-17T15:23:05.335620896-06:00","close_reason":"Epic complete: All tasks implemented and tested - platform detection, requirements parsing, code2prompt integration, prd.json generation, platform adaptation"}
{"id":"TO-yb6","title":"Configure environment variables","description":"Set environment variables: HF_HOME, REPO_PATH, INDEX_DIR, CKPT_DIR, CUDA_HOME=/usr/local/cuda-13.0. Also configure ToolOrchestra port range 18000-18999 (VLLM=18000, TAU2_API=18010, RAY=18020, WANDB=18030).","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:44:43.13395038-06:00","created_by":"kp","updated_at":"2026-01-17T02:28:14.204657583-06:00","closed_at":"2026-01-17T02:28:14.204657583-06:00","close_reason":"Environment variables configured: HF_HOME, REPO_PATH, INDEX_DIR, CKPT_DIR, CUDA_HOME, and port configuration 18000-18999"}
