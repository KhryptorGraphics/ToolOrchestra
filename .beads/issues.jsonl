{"id":"TO-017","title":"Install project packages (editable)","description":"pip install -e tau2-bench and training/rollout","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:42.947832361-06:00","created_by":"kp","updated_at":"2026-01-17T02:38:32.772038106-06:00","closed_at":"2026-01-17T02:38:32.772038106-06:00","close_reason":"Project packages installed: tau2, verl modules import successfully. CLI requires REPO_PATH env var which is configured in bashrc.","dependencies":[{"issue_id":"TO-017","depends_on_id":"TO-jeu","type":"blocks","created_at":"2026-01-15T14:44:42.948977045-06:00","created_by":"kp"}]}
{"id":"TO-11e","title":"Final verification","description":"Comprehensive CUDA 13.0 verification across all 3 environments: toolorchestra (PyTorch, Flash Attention, vLLM, CuPy, tau2, verl), vllm1 (PyTorch, Flash Attention, vLLM), retriever (already working). Tests GPU memory, CUDA kernels, and environment variables.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-15T14:44:43.195379218-06:00","created_by":"kp","updated_at":"2026-01-17T02:42:04.517822826-06:00","closed_at":"2026-01-17T02:42:04.517822826-06:00","close_reason":"All verification passed: toolorchestra (PyTorch 2.9.1 CUDA 13.0, Flash Attention, vLLM, CuPy, Core ML, tau2/verl), vllm1 (PyTorch CUDA 13.0, Flash Attention, vLLM), retriever (PyTorch CUDA 13.0, FAISS). GPU computation and Flash Attention kernels verified. Env vars in ~/.profile.","dependencies":[{"issue_id":"TO-11e","depends_on_id":"TO-jeu","type":"blocks","created_at":"2026-01-17T02:21:23.235974438-06:00","created_by":"daemon"},{"issue_id":"TO-11e","depends_on_id":"TO-yb6","type":"blocks","created_at":"2026-01-17T02:21:24.683793976-06:00","created_by":"daemon"}]}
{"id":"TO-16s","title":"Install PyTorch CUDA 13.0 in toolorchestra","description":"Install PyTorch 2.9.1, torchvision 0.24.1, torchaudio 2.9.1 from Jetson AI Lab mirror (https://pypi.jetson-ai-lab.io/sbsa/cu130/) in toolorchestra environment. This replaces the CPU-only PyTorch currently installed.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-17T02:20:40.802019796-06:00","updated_at":"2026-01-17T02:23:30.247537358-06:00","closed_at":"2026-01-17T02:23:30.247537358-06:00","close_reason":"PyTorch 2.9.1 with CUDA 13.0 installed and verified. Device: NVIDIA Thor, Compute Capability: (11, 0)"}
{"id":"TO-2lp","title":"Build vLLM from source","description":"Clone and build vLLM v0.8.5 for ARM64","acceptance_criteria":"from vllm import LLM works","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:44:28.585293358-06:00","created_by":"kp","updated_at":"2026-01-15T19:56:35.631578344-06:00","closed_at":"2026-01-15T19:56:35.631578344-06:00","close_reason":"vLLM v0.14.0rc2 built from main branch for CUDA 13.0 / PyTorch 2.9.1"}
{"id":"TO-5w4","title":"Create retriever conda environment","description":"Create env with Python 3.12, PyTorch, FAISS, and retrieval packages","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:43.012680912-06:00","created_by":"kp","updated_at":"2026-01-15T20:57:52.343726929-06:00","closed_at":"2026-01-15T20:57:52.343726929-06:00","close_reason":"Retriever environment created with faiss-cpu, pyserini, tavily, torch 2.9.1, transformers 4.57.5"}
{"id":"TO-92i","title":"Fix conda pip path issue and install PyTorch","description":"Install PyTorch with CUDA support for Jetson Thor. Use explicit conda pip path to avoid system pip.","acceptance_criteria":"torch.cuda.is_available() returns True","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-15T14:44:28.461863514-06:00","created_by":"kp","updated_at":"2026-01-15T14:58:56.807261291-06:00","closed_at":"2026-01-15T14:58:56.807261291-06:00","close_reason":"PyTorch 2.9.1+cu130 installed successfully. CUDA available: True. GPU: NVIDIA Thor"}
{"id":"TO-cet","title":"Fix PyTorch CUDA 13.0 in vllm1 environment","description":"Uninstall CPU PyTorch, install PyTorch 2.9.1, torchvision 0.24.1, torchaudio 2.9.1 with CUDA 13.0 from Jetson AI Lab mirror in vllm1 environment.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-17T02:20:42.266262186-06:00","updated_at":"2026-01-17T02:23:31.721373915-06:00","closed_at":"2026-01-17T02:23:31.721373915-06:00","close_reason":"PyTorch 2.9.1 with CUDA 13.0 installed and verified in vllm1. Device: NVIDIA Thor"}
{"id":"TO-jeu","title":"Install core ML packages in toolorchestra","description":"Install core ML packages with CuPy 13.6.0 from Jetson AI Lab mirror, and filtered requirements.txt (excluding torch, flash-attn, vllm, cupy, langfuse, xformers, nvidia-*). Packages: transformers, accelerate, datasets, ray, wandb, hydra-core.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:28.646265203-06:00","created_by":"kp","updated_at":"2026-01-17T02:35:40.396624886-06:00","closed_at":"2026-01-17T02:35:40.396624886-06:00","close_reason":"Core ML packages verified: transformers 4.57.5, accelerate 1.12.0, datasets 4.5.0, ray 2.52.1, wandb 0.24.0, hydra 1.3.2, cupy-cuda12x 13.6.0","dependencies":[{"issue_id":"TO-jeu","depends_on_id":"TO-mjt","type":"blocks","created_at":"2026-01-17T02:21:21.783445554-06:00","created_by":"daemon"}]}
{"id":"TO-ky4","title":"Build Flash Attention from source","description":"Clone and build flash-attn for ARM64 SM 90","acceptance_criteria":"from flash_attn import flash_attn_func works","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:28.525186603-06:00","created_by":"kp","updated_at":"2026-01-15T20:00:07.186069592-06:00","closed_at":"2026-01-15T20:00:07.186069592-06:00","close_reason":"Flash Attention v2.7.4.post1 built from source for CUDA 13.0"}
{"id":"TO-mjt","title":"Install vLLM 0.13.0+cu130 in toolorchestra and vllm1","description":"Uninstall old vLLM from vllm1 (v0.10.0), install pre-built vllm==0.13.0+cu130 from Jetson AI Lab mirror in both toolorchestra and vllm1 environments.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T02:20:45.178082491-06:00","updated_at":"2026-01-17T02:28:12.659542236-06:00","closed_at":"2026-01-17T02:28:12.659542236-06:00","close_reason":"vLLM 0.13.0+cu130 installed and verified in both toolorchestra and vllm1 with CUDA support","dependencies":[{"issue_id":"TO-mjt","depends_on_id":"TO-rwb","type":"blocks","created_at":"2026-01-17T02:21:20.281431708-06:00","created_by":"daemon"}]}
{"id":"TO-qff","title":"Create vllm1 conda environment","description":"Create inference env with transformers\u003c4.54 for tau2-bench","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-15T14:44:43.073061716-06:00","created_by":"kp","updated_at":"2026-01-15T20:43:24.147256597-06:00","closed_at":"2026-01-15T20:43:24.147256597-06:00","close_reason":"vllm1 environment created with PyTorch 2.7.0, transformers 4.53.3, vLLM 0.10.0"}
{"id":"TO-rwb","title":"Install Flash Attention 2.8.4 in toolorchestra and vllm1","description":"Uninstall existing flash-attn (built against CPU PyTorch), install pre-built flash-attn==2.8.4 from Jetson AI Lab mirror in both toolorchestra and vllm1 environments.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-17T02:20:43.727364784-06:00","updated_at":"2026-01-17T02:25:15.71735816-06:00","closed_at":"2026-01-17T02:25:15.71735816-06:00","close_reason":"Flash Attention 2.8.4 installed and verified in both toolorchestra and vllm1 with CUDA support","dependencies":[{"issue_id":"TO-rwb","depends_on_id":"TO-16s","type":"blocks","created_at":"2026-01-17T02:21:17.295024881-06:00","created_by":"daemon"},{"issue_id":"TO-rwb","depends_on_id":"TO-cet","type":"blocks","created_at":"2026-01-17T02:21:18.780108118-06:00","created_by":"daemon"}]}
{"id":"TO-yb6","title":"Configure environment variables","description":"Set environment variables: HF_HOME, REPO_PATH, INDEX_DIR, CKPT_DIR, CUDA_HOME=/usr/local/cuda-13.0. Also configure ToolOrchestra port range 18000-18999 (VLLM=18000, TAU2_API=18010, RAY=18020, WANDB=18030).","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:44:43.13395038-06:00","created_by":"kp","updated_at":"2026-01-17T02:28:14.204657583-06:00","closed_at":"2026-01-17T02:28:14.204657583-06:00","close_reason":"Environment variables configured: HF_HOME, REPO_PATH, INDEX_DIR, CKPT_DIR, CUDA_HOME, and port configuration 18000-18999"}
